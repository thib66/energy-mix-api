import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler
import warnings
warnings.filterwarnings("ignore")
TARGETS = ["thermique", "nucleaire", "eolien", "solaire", "hydraulique", "bioenergies"]
LAGS = [1, 2, 4, 8, 56]
SEQUENCE_LENGTH = 168  # 7 jours



# Chargement des donn√©es
def load_data():
    file_name="dataset_final.csv"

    df = pd.read_csv(file_name, parse_dates=["date_time"])

    # üõ† Remplacement des valeurs manquantes par interpolation lin√©aire
    df["vitesse_vent"] = df["vitesse_vent"].interpolate(method="linear")
    df["temperature"] = df["temperature"].interpolate(method="linear")

    # Extraction des features temporelles
    df["hour"] = df["date_time"].dt.hour
    df["day_of_week"] = df["date_time"].dt.weekday
    df["month"] = df["date_time"].dt.month
    df["day_of_year"] = df["date_time"].dt.dayofyear
    df["week_of_year"] = df["date_time"].dt.isocalendar().week

    # Encodage sinuso√Ødal des variables temporelles
    df["sin_hour"] = np.sin(2 * np.pi * df["hour"] / 24)
    df["cos_hour"] = np.cos(2 * np.pi * df["hour"] / 24)
    df["sin_day_of_week"] = np.sin(2 * np.pi * df["day_of_week"] / 7)
    df["cos_day_of_week"] = np.cos(2 * np.pi * df["day_of_week"] / 7)
    df["sin_month"] = np.sin(2 * np.pi * df["month"] / 12)
    df["cos_month"] = np.cos(2 * np.pi * df["month"] / 12)

    # Suppression des colonnes inutiles
    columns_to_drop = ['region', 'n_station', 'nom_station', 'latitude', 'longitude', "hour", "day_of_week", "month", "day_of_year", "week_of_year"]
    df.drop(columns=columns_to_drop, inplace=True, errors='ignore')

    # Mise en index et r√©√©chantillonnage
    df.set_index('date_time', inplace=True)
    df = df.resample("3h").mean()

    return df

# Pr√©traitement des donn√©es
def preprocess_data(df):
    # Ajout des moyennes et √©carts-types glissants
    window_size = 56
    for target in TARGETS:
        df[f"{target}_rolling_mean_{window_size}"] = df[target].rolling(window=window_size).mean()
        df[f"{target}_rolling_std_{window_size}"] = df[target].rolling(window=window_size).std()

    df = df.dropna()

    # Cr√©ation des LAGS
    for lag in LAGS:
        for target in TARGETS:
            df[f"{target}_lag{lag}"] = df[target].shift(lag)
    df = df.dropna()

    # Mise √† jour des features
    features = [col for col in df.columns if col not in TARGETS]
    for target in TARGETS:
        features.append(f"{target}_rolling_mean_{window_size}")
        features.append(f"{target}_rolling_std_{window_size}")

    # Suppression des features √† faible variance
    low_variance_features = df[features].std()
    variance_threshold = 0.01
    features = low_variance_features[low_variance_features > variance_threshold].index.tolist()

    # S√©lection des features les plus corr√©l√©es aux TARGETS
    correlation_matrix = df[features + TARGETS].corr()
    correlation_target = correlation_matrix[TARGETS].abs().mean(axis=1)
    features = correlation_target[correlation_target > 0.1].index.tolist()

    return df, features

# Normalisation
def normalize_data(df, features):
    scaler_X = RobustScaler()
    scaler_y = RobustScaler()

    df[features] = scaler_X.fit_transform(df[features])
    df[TARGETS] = scaler_y.fit_transform(df[TARGETS])

    return df, scaler_X, scaler_y

# Cr√©ation des s√©quences
def create_sequences(data, TARGETS, features, seq_length, stride=3):
    X, y = [], []
    for i in range(0, len(data) - seq_length, stride):
        X.append(data.iloc[i:i + seq_length][features].values)
        y.append(data.iloc[i + seq_length][TARGETS].values)
    return np.array(X), np.array(y)



